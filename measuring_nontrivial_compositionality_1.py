# -*- coding: utf-8 -*-
"""measuring_nontrivial_compositionality_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EfUBj1MsIG1RKNgkmci-bBrv_YdbkzQe
"""

import random
from itertools import product
import string
from typing import Callable, List, Dict, Tuple, Iterable
from collections import defaultdict

# !pip install editdistance scipy torch &>/dev/null
import numpy as np
from scipy.stats import spearmanr
from scipy.spatial.distance import hamming
import torch
import editdistance

POSSIBLE_COLORS = ['blue', 'green', 'gold', 'yellow', 'red', 'orange', 'black', 'white']
POSSIBLE_SHAPES = ['square', 'circle', 'ellipse', 'triangle', 'rectangle', 'pentagon', 'hexagon', 'cross']
Protocol = Dict[Tuple[str, str], str]

def get_trivially_compositional_protocol(num_colors: int, num_shapes: int) -> Protocol:
    assert num_colors < 9 and num_shapes < 9
    objects = product(POSSIBLE_COLORS[:num_colors], POSSIBLE_SHAPES[:num_shapes])
    alphabet = list(string.ascii_letters[:num_colors+num_shapes])
    random.shuffle(alphabet)
    color_names, shape_names = alphabet[:num_colors], alphabet[num_colors:]
    color_mapping = {color: color_name for color, color_name 
                     in zip(POSSIBLE_COLORS[:num_colors], color_names)}
    shape_mapping = {color: color_name for color, color_name 
                     in zip(POSSIBLE_SHAPES[:num_shapes], shape_names)}
    mapping = {}
    for color, shape in objects:
        mapping[(color, shape)] = ''.join((color_mapping[color], shape_mapping[shape]))
    return mapping

get_trivially_compositional_protocol(5, 5)

def get_nontrivially_compositional_protocol(num_colors: int, num_shapes: int) -> Protocol:
    assert num_colors < 9 and num_shapes < 9
    num_letters = num_colors + num_shapes
    alphabet = list(string.ascii_letters[:num_letters])
    random.shuffle(alphabet)
    mapping = {}
    for i, color in enumerate(POSSIBLE_COLORS[:num_colors]):
        for j, shape in enumerate(POSSIBLE_SHAPES[:num_shapes]):
            first_letter = alphabet[(i - j) % num_letters]
            second_letter = alphabet[(i + j) % num_letters]
            mapping[color, shape] = first_letter + second_letter
    return mapping

get_nontrivially_compositional_protocol(5, 5)

def get_holistic_protocol(num_colors: int, num_shapes: int) -> Protocol:
    objects = product(POSSIBLE_COLORS[:num_colors], POSSIBLE_SHAPES[:num_shapes])
    alphabet = string.ascii_letters[:num_colors + num_shapes]
    object_names = list(product(alphabet, alphabet))
    random.shuffle(object_names)
    mapping = {}
    for (color, shape), name in zip(objects, object_names):
        mapping[(color, shape)] = ''.join(name)
    return mapping

get_holistic_protocol(5, 5)

def get_random_protocol(num_colors: int, num_shapes: int) -> Protocol:
    objects = product(POSSIBLE_COLORS[:num_colors], POSSIBLE_SHAPES[:num_shapes])
    alphabet = string.ascii_letters[:num_colors + num_shapes]
    mapping = {}
    for color, shape in objects:
        mapping[(shape, color)] = ''.join([random.choice(alphabet), random.choice(alphabet)])
    return mapping

get_random_protocol(5, 5)

class TopographicSimilarity:

    def __init__(self, input_metric: Callable, messages_metric: Callable):
        self.input_metric = input_metric
        self.messages_metric = messages_metric

    def measure(self, protocol: Protocol) -> float:
        distance_messages = self._compute_distances(
            sequence=list(protocol.values()), 
            metric=self.messages_metric)
        distance_inputs = self._compute_distances(
            sequence=list(protocol.keys()), 
            metric=self.input_metric)
        return spearmanr(distance_messages, distance_inputs).correlation

    def _compute_distances(self, sequence: List[str], metric: Callable) -> List[float]:
        distances = []
        for i, element_1 in enumerate(sequence):
            for j, element_2 in enumerate(sequence[i+1:]):
                distances.append(metric(element_1, element_2))
        return distances

protocol = get_random_protocol(5, 5)
similarity = TopographicSimilarity(
    input_metric=hamming,
    messages_metric=editdistance.eval)
similarity.measure(protocol)

class ContextIndependence:

    def __init__(self, num_colors: int, num_shapes: int):
        self.num_colors = num_colors
        self.num_shapes = num_shapes
        self.num_concepts = num_colors + num_shapes

    def measure(self, protocol: Protocol) -> float:
        character_set = set(c for message in protocol.values() for c in message)
        vocab = {char: idx for idx, char in enumerate(character_set)}
        concept_set = set(concept for concepts in protocol.keys() for concept in concepts)
        concepts = {concept: idx for idx, concept in enumerate(concept_set)}        
        
        concept_symbol_matrix = self._compute_concept_symbol_matrix(protocol, vocab, concepts)
        v_cs = concept_symbol_matrix.argmax(axis=1)
        context_independence_scores = np.zeros(len(concept_set))
        for concept in range(len(concept_set)):
            v_c = v_cs[concept]
            p_vc_c = concept_symbol_matrix[concept, v_c] / concept_symbol_matrix[concept, :].sum(axis=0)
            p_c_vc = concept_symbol_matrix[concept, v_c] / concept_symbol_matrix[:, v_c].sum(axis=0)
            context_independence_scores[concept] = p_vc_c * p_c_vc
        return context_independence_scores.mean(axis=0)
    
    def _compute_concept_symbol_matrix(
        self, 
        protocol: Protocol, 
        vocab: Dict[str, int], 
        concepts: Dict[str, int],
        epsilon: float = 10e-8
    ) -> np.ndarray:
        concept_to_message = defaultdict(list)
        for (color, shape), message in protocol.items():
            concept_to_message[color] += list(message)
            concept_to_message[shape] += list(message)
        concept_symbol_matrix = np.ndarray((self.num_concepts, len(vocab)))
        concept_symbol_matrix.fill(epsilon)
        for concept, symbols in concept_to_message.items():
            for symbol in symbols:
                concept_symbol_matrix[concepts[concept], vocab[symbol]] += 1
        return concept_symbol_matrix

protocol = get_holistic_protocol(10, 10)
ci = ContextIndependence(10, 10)
np.set_printoptions(precision=1, suppress=True)


class AdditiveComposition(torch.nn.Module):

    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        return x + y


class LinearComposition(torch.nn.Module):
    def __init__(self, representation_size: int):
        super().__init__()
        self.linear = torch.nn.Linear(representation_size * 2, representation_size)

    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        return self.linear(torch.cat((x, y), dim=1))


class MLPComposition(torch.nn.Module):
    def __init__(self, representation_size: int):
        super().__init__()
        self.linear_1 = torch.nn.Linear(representation_size * 2, 50)
        self.linear_2 = torch.nn.Linear(50, representation_size)

    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        return self.linear_2(torch.tanh(self.linear_1(torch.cat((x, y), dim=1))))


class MultiplicativeComposition(torch.nn.Module):
    def __init__(self, representation_size: int):
        super().__init__()
        self.bilinear = torch.nn.Bilinear(
            in1_features=representation_size,
            in2_features=representation_size,
            out_features=representation_size
        )

    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        return self.bilinear(x, y)


class CustomLoss(torch.nn.Module):

    def forward(self, y, yhat):
        yhat = yhat.unsqueeze(dim=0)
        first, second = y[:, :8], y[:, 8:]
        first_target, second_target = yhat[:, :8].argmax(keepdim=True).unsqueeze(dim=0), yhat[:, 8:].argmax(keepdim=True).unsqueeze(dim=0)
        return torch.nn.functional.cross_entropy(first, first_target) + torch.nn.functional.cross_entropy(second,
                                                                                                          second_target)


class Objective(torch.nn.Module):
    def __init__(
            self,
            num_concepts: int,
            vocab_size: int,
            message_length: int,
            composition_fn: torch.nn.Module,
            loss_fn: torch.nn.Module,
            zero_init=True
    ):
        super().__init__()
        self.composition_fn = composition_fn
        self.loss_fn = loss_fn
        self.emb = torch.nn.Embedding(num_concepts, message_length * vocab_size)
        if zero_init:
            self.emb.weight.data.zero_()

    def compose(self, derivation):
        if isinstance(derivation, tuple):
            args = (self.compose(node) for node in derivation)
            return self.composition_fn(*args)
        else:
            return self.emb(derivation)

    def forward(self, message, derivation):
        return self.loss_fn(self.compose(derivation), message)


class TreeReconstructionError:

    def __init__(self, num_colors: int, num_shapes: int, message_length: int):
        self.num_colors = num_colors
        self.num_shapes = num_shapes
        self.num_concepts = num_colors + num_shapes
        self.message_length = message_length

    def measure(self, protocol: Protocol) -> float:
        tensorised_protocol = self._protocol_to_tensor(protocol)
        vocab = self._get_vocab(protocol)
        objective = Objective(
            num_concepts=self.num_concepts,
            vocab_size=len(vocab),
            message_length=self.message_length,
            composition_fn=MLPComposition(representation_size=self.message_length * len(vocab)),
            loss_fn=CustomLoss()
        )
        reconstruction_error = self._train_model(
            X=tensorised_protocol.values(),
            y=tensorised_protocol.keys(),
            objective=objective,
            optimizer=torch.optim.Adam(objective.parameters(), lr=5e-4, weight_decay=0.0001),
            n_epochs=20_000
        )
        return reconstruction_error

    def _to_batch(self, X: Iterable[torch.Tensor], y: Iterable[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:
        X = torch.stack(tuple(X))
    def _train_model(
            self,
            X: Iterable[torch.Tensor],
            y: Iterable[torch.Tensor],
            objective: torch.nn.Module,
            optimizer: torch.optim.Optimizer,
            n_epochs: int,
            quiet: bool = True
    ) -> float:
        for t in range(n_epochs):
            optimizer.zero_grad()
            errors = [objective(message, derivation) for message, derivation in zip(X, y)]
            loss = sum(errors)
            loss.backward()
            if not quiet and t % 1000 == 0:
                print(f'Training loss at epoch {t} is {loss.item():.4f}')
            optimizer.step()
        return loss.item()

    def _protocol_to_tensor(self, protocol: Protocol) -> Dict[
        Tuple[torch.LongTensor, torch.LongTensor], torch.LongTensor]:
        vocab = self._get_vocab(protocol)
        concept_set = set(concept for concepts in protocol.keys() for concept in concepts)
        concepts = {concept: idx for idx, concept in enumerate(concept_set)}
        tensorized_protocol = {}
        for (color, shape), message in protocol.items():
            color = torch.LongTensor([concepts[color]])
            shape = torch.LongTensor([concepts[shape]])
            message = torch.LongTensor([vocab[char] for char in message])
            tensorized_protocol[color, shape] = torch.nn.functional.one_hot(
                message, num_classes=len(vocab)).reshape(-1)
        return tensorized_protocol

    def _get_vocab(self, protocol: Protocol) -> Dict[str, int]:
        character_set = set(c for message in protocol.values() for c in message)
        return {char: idx for idx, char in enumerate(character_set)}


NUM_COLORS = NUM_SHAPES = 4
PROTOCOLS = {
    # 'holistic': get_holistic_protocol(NUM_COLORS, NUM_SHAPES),
    'trivial': get_trivially_compositional_protocol(NUM_COLORS, NUM_SHAPES),
    # 'random': get_random_protocol(NUM_COLORS, NUM_SHAPES),
    'nontrivial': get_nontrivially_compositional_protocol(NUM_COLORS, NUM_SHAPES),
}

for name, protocol in PROTOCOLS.items():
    print(name)
    tre = TreeReconstructionError(NUM_COLORS, NUM_SHAPES, 2)
    print(f'tre = {tre.measure(protocol):.4f}')
    ci = ContextIndependence(NUM_COLORS, NUM_SHAPES)
    print(f'ci = {ci.measure(protocol):.4f}')
    topo = TopographicSimilarity(
        input_metric=hamming,
        messages_metric=editdistance.eval
    )
    print(f'topo = {topo.measure(protocol):.4f}')


