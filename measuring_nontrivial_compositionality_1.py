# -*- coding: utf-8 -*-
"""measuring_nontrivial_compositionality_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EfUBj1MsIG1RKNgkmci-bBrv_YdbkzQe
"""

import random
from itertools import product
import string
from typing import Callable, List, Dict, Tuple, Iterable, Type
from collections import defaultdict

# !pip install editdistance scipy torch tqdm &>/dev/null
import numpy as np
from scipy.stats import spearmanr
from scipy.spatial.distance import hamming
import pandas as pd
import torch
import editdistance
from tqdm import trange

POSSIBLE_COLORS = ['blue', 'green', 'gold', 'yellow', 'red', 'orange', 'black', 'white']
POSSIBLE_SHAPES = ['square', 'circle', 'ellipse', 'triangle', 'rectangle', 'pentagon', 'hexagon', 'cross']
Protocol = Dict[Tuple[str, str], str]

def get_trivially_compositional_protocol(num_colors: int, num_shapes: int) -> Protocol:
    assert num_colors < 9 and num_shapes < 9
    objects = product(POSSIBLE_COLORS[:num_colors], POSSIBLE_SHAPES[:num_shapes])
    alphabet = list(string.ascii_letters[:num_colors+num_shapes])
    random.shuffle(alphabet)
    color_names, shape_names = alphabet[:num_colors], alphabet[num_colors:]
    color_mapping = {color: color_name for color, color_name 
                     in zip(POSSIBLE_COLORS[:num_colors], color_names)}
    shape_mapping = {color: color_name for color, color_name 
                     in zip(POSSIBLE_SHAPES[:num_shapes], shape_names)}
    mapping = {}
    for color, shape in objects:
        mapping[(color, shape)] = ''.join((color_mapping[color], shape_mapping[shape]))
    return mapping

get_trivially_compositional_protocol(5, 5)

def get_nontrivially_compositional_protocol(num_colors: int, num_shapes: int) -> Protocol:
    assert num_colors < 9 and num_shapes < 9
    num_letters = num_colors + num_shapes
    alphabet = list(string.ascii_letters[:num_letters])
    random.shuffle(alphabet)
    mapping = {}
    for i, color in enumerate(POSSIBLE_COLORS[:num_colors]):
        for j, shape in enumerate(POSSIBLE_SHAPES[:num_shapes]):
            first_letter = alphabet[(i - j) % num_letters]
            second_letter = alphabet[(i + j) % num_letters]
            mapping[color, shape] = first_letter + second_letter
    return mapping

get_nontrivially_compositional_protocol(5, 5)

def get_holistic_protocol(num_colors: int, num_shapes: int) -> Protocol:
    objects = product(POSSIBLE_COLORS[:num_colors], POSSIBLE_SHAPES[:num_shapes])
    alphabet = string.ascii_letters[:num_colors + num_shapes]
    object_names = list(product(alphabet, alphabet))
    random.shuffle(object_names)
    mapping = {}
    for (color, shape), name in zip(objects, object_names):
        mapping[(color, shape)] = ''.join(name)
    return mapping

get_holistic_protocol(5, 5)

def get_random_protocol(num_colors: int, num_shapes: int) -> Protocol:
    objects = product(POSSIBLE_COLORS[:num_colors], POSSIBLE_SHAPES[:num_shapes])
    alphabet = string.ascii_letters[:num_colors + num_shapes]
    mapping = {}
    for color, shape in objects:
        mapping[(shape, color)] = ''.join([random.choice(alphabet), random.choice(alphabet)])
    return mapping

get_random_protocol(5, 5)

class TopographicSimilarity:

    def __init__(self, input_metric: Callable, messages_metric: Callable):
        self.input_metric = input_metric
        self.messages_metric = messages_metric

    def measure(self, protocol: Protocol) -> float:
        distance_messages = self._compute_distances(
            sequence=list(protocol.values()), 
            metric=self.messages_metric)
        distance_inputs = self._compute_distances(
            sequence=list(protocol.keys()), 
            metric=self.input_metric)
        return spearmanr(distance_messages, distance_inputs).correlation

    def _compute_distances(self, sequence: List[str], metric: Callable) -> List[float]:
        distances = []
        for i, element_1 in enumerate(sequence):
            for j, element_2 in enumerate(sequence[i+1:]):
                distances.append(metric(element_1, element_2))
        return distances

protocol = get_random_protocol(5, 5)
similarity = TopographicSimilarity(
    input_metric=hamming, 
    messages_metric=editdistance.eval)
similarity.measure(protocol)

class ContextIndependence:

    def __init__(self, num_colors: int, num_shapes: int):
        self.num_colors = num_colors
        self.num_shapes = num_shapes
        self.num_concepts = num_colors + num_shapes

    def measure(self, protocol: Protocol) -> float:
        character_set = set(c for message in protocol.values() for c in message)
        vocab = {char: idx for idx, char in enumerate(character_set)}
        concept_set = set(concept for concepts in protocol.keys() for concept in concepts)
        concepts = {concept: idx for idx, concept in enumerate(concept_set)}        
        
        concept_symbol_matrix = self._compute_concept_symbol_matrix(protocol, vocab, concepts)
        v_cs = concept_symbol_matrix.argmax(axis=1)
        context_independence_scores = np.zeros(len(concept_set))
        for concept in range(len(concept_set)):
            v_c = v_cs[concept]
            p_vc_c = concept_symbol_matrix[concept, v_c] / concept_symbol_matrix[concept, :].sum(axis=0)
            p_c_vc = concept_symbol_matrix[concept, v_c] / concept_symbol_matrix[:, v_c].sum(axis=0)
            context_independence_scores[concept] = p_vc_c * p_c_vc
        return context_independence_scores.mean(axis=0)
    
    def _compute_concept_symbol_matrix(
        self, 
        protocol: Protocol, 
        vocab: Dict[str, int], 
        concepts: Dict[str, int],
        epsilon: float = 10e-8
    ) -> np.ndarray:
        concept_to_message = defaultdict(list)
        for (color, shape), message in protocol.items():
            concept_to_message[color] += list(message)
            concept_to_message[shape] += list(message)
        concept_symbol_matrix = np.ndarray((self.num_concepts, len(vocab)))
        concept_symbol_matrix.fill(epsilon)
        for concept, symbols in concept_to_message.items():
            for symbol in symbols:
                concept_symbol_matrix[concepts[concept], vocab[symbol]] += 1
        return concept_symbol_matrix

protocol = get_holistic_protocol(10, 10)
ci = ContextIndependence(10, 10)
np.set_printoptions(precision=1, suppress=True)
print(protocol)
print(ci.measure(protocol))

class CompositionFunction(torch.nn.Module):

    def __init__(self, representation_size: int):
        super().__init__()

    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        raise NotImplemented


class AdditiveComposition(CompositionFunction):

    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        return x + y


class LinearComposition(CompositionFunction):
    def __init__(self, representation_size: int):
        super().__init__(representation_size)
        self.linear = torch.nn.Linear(representation_size * 2, representation_size)

    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        return self.linear(torch.cat((x, y), dim=1))


class MLPComposition(CompositionFunction):
    def __init__(self, representation_size: int):
        super().__init__(representation_size)
        self.linear_1 = torch.nn.Linear(representation_size * 2, 50)
        self.linear_2 = torch.nn.Linear(50, representation_size)

    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        return self.linear_2(torch.tanh(self.linear_1(torch.cat((x, y), dim=1))))


class LinearMultiplicationComposition(CompositionFunction):
    def __init__(self, representation_size: int):
        super().__init__(representation_size)
        self.linear_1 = torch.nn.Linear(representation_size, representation_size)
        self.linear_2 = torch.nn.Linear(representation_size, representation_size)

    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        return self.linear_1(x) * self.linear_2(y)


class MultiplicativeComposition(CompositionFunction):
    def __init__(self, representation_size: int):
        super().__init__(representation_size)
        self.bilinear = torch.nn.Bilinear(
            in1_features=representation_size,
            in2_features=representation_size,
            out_features=representation_size
        )

    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        return self.bilinear(x, y)


class DoubleCrossEntropyLoss(torch.nn.Module):

    def __init__(self, message_length: int):
        super().__init__()
        assert message_length == 2, 'message_length != 2 not implemented yet'
        self.message_length = message_length

    def forward(self, reconstruction, message):
        middle = int(self.message_length/2)
        first, second = reconstruction[:, :middle], reconstruction[:, middle:]
        first_target, second_target = message[:, :middle].argmax(dim=1), message[:, middle:].argmax(dim=1)
        first_loss = torch.nn.functional.cross_entropy(first, first_target)
        second_loss = torch.nn.functional.cross_entropy(second, second_target)
        return first_loss + second_loss


class Objective(torch.nn.Module):
    def __init__(
            self,
            num_concepts: int,
            vocab_size: int,
            message_length: int,
            composition_fn: torch.nn.Module,
            loss_fn: torch.nn.Module,
            zero_init=True
    ):
        super().__init__()
        self.composition_fn = composition_fn
        self.loss_fn = loss_fn
        self.emb = torch.nn.Embedding(num_concepts, message_length * vocab_size)
        if zero_init:
            self.emb.weight.data.zero_()

    def compose(self, derivations):
        if isinstance(derivations, tuple):
            args = (self.compose(node) for node in derivations)
            return self.composition_fn(*args)
        else:
            return self.emb(derivations)

    def forward(self, messages, derivations):
        return self.loss_fn(self.compose(derivations), messages)


class TreeReconstructionError:

    def __init__(
            self,
            num_colors: int,
            num_shapes: int,
            message_length: int,
            composition_fn: Type[CompositionFunction],
    ):
        self.num_colors = num_colors
        self.num_shapes = num_shapes
        self.num_concepts = num_colors + num_shapes
        self.message_length = message_length
        self.composition_fn = composition_fn

    def measure(self, protocol: Protocol) -> float:
        tensorised_protocol = self._protocol_to_tensor(protocol)
        vocab = self._get_vocab(protocol)
        objective = Objective(
            num_concepts=self.num_concepts,
            vocab_size=len(vocab),
            message_length=self.message_length,
            composition_fn=self.composition_fn(representation_size=self.message_length * len(vocab)),
            loss_fn=DoubleCrossEntropyLoss(message_length=self.message_length)
        )
        messages, derivations = self._to_batch(tensorised_protocol.values(), tensorised_protocol.keys())
        reconstruction_error = self._train_model(
            messages=messages,
            derivations=derivations,
            objective=objective,
            optimizer=torch.optim.Adam(objective.parameters(), lr=1e-3, weight_decay=0.0001),
            n_epochs=20_000
        )
        return reconstruction_error

    def _to_batch(
            self,
            messages: Iterable[torch.Tensor],
            derivations: Iterable[torch.Tensor]
    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        messages = torch.stack(tuple(messages))
        left_derivations, right_derivations = zip(*derivations)
        left_derivations, right_derivations = torch.cat(tuple(left_derivations)), torch.cat(tuple(right_derivations))
        return messages, (left_derivations, right_derivations)

    def _train_model(
            self,
            messages: Iterable[torch.Tensor],
            derivations: Iterable[torch.Tensor],
            objective: torch.nn.Module,
            optimizer: torch.optim.Optimizer,
            n_epochs: int,
            quiet: bool = True
    ) -> float:
        for t in range(n_epochs):
            optimizer.zero_grad()
            loss = objective(messages, derivations).sum()
            # errors = [objective(message, derivation) for message, derivation in zip(message, derivation)]
            # loss = sum(errors)
            loss.backward()
            if not quiet and t % 1000 == 0:
                print(f'Training loss at epoch {t} is {loss.item():.4f}')
            optimizer.step()
        return loss.item()

    def _protocol_to_tensor(self, protocol: Protocol) -> Dict[
        Tuple[torch.LongTensor, torch.LongTensor], torch.LongTensor]:
        vocab = self._get_vocab(protocol)
        concept_set = set(concept for concepts in protocol.keys() for concept in concepts)
        concepts = {concept: idx for idx, concept in enumerate(concept_set)}
        tensorized_protocol = {}
        for (color, shape), message in protocol.items():
            color = torch.LongTensor([concepts[color]])
            shape = torch.LongTensor([concepts[shape]])
            message = torch.LongTensor([vocab[char] for char in message])
            tensorized_protocol[color, shape] = torch.nn.functional.one_hot(
                message, num_classes=len(vocab)).reshape(-1)
        return tensorized_protocol

    def _get_vocab(self, protocol: Protocol) -> Dict[str, int]:
        character_set = set(c for message in protocol.values() for c in message)
        return {char: idx for idx, char in enumerate(character_set)}


NUM_COLORS = NUM_SHAPES = 5
df = pd.DataFrame(columns=['protocol', 'metric', 'value', 'seed'])

for seed in trange(1):
    protocols = {
    'holistic': get_holistic_protocol(NUM_COLORS, NUM_SHAPES),
    'trivially compositional': get_trivially_compositional_protocol(NUM_COLORS, NUM_SHAPES),
    'random': get_random_protocol(NUM_COLORS, NUM_SHAPES),
    'non-trivially compositional': get_nontrivially_compositional_protocol(NUM_COLORS, NUM_SHAPES),
    }
    for name, protocol in PROTOCOLS.items():
        # print(name)
        tre1 = TreeReconstructionError(NUM_COLORS, NUM_SHAPES, 2, LinearComposition)
        tre1_value = tre.measure(protocol)
        # print(f'tre = {tre_value:.4f}')
        df.loc[len(df)] = [name, 'TRE with linear composition', -tre1_value, seed]

        tre2 = TreeReconstructionError(NUM_COLORS, NUM_SHAPES, 2, AdditiveComposition)
        tre2_value = tre2.measure(protocol)
        # print(f'tre = {tre_value:.4f}')
        df.loc[len(df)] = [name, 'TRE with additive composition', -tre2_value, seed]

        ci = ContextIndependence(NUM_COLORS, NUM_SHAPES)
        ci_value = ci.measure(protocol)
        # print(f'ci = {ci_value:.4f}')
        df.loc[len(df)] = [name, 'context independence', ci_value, seed]

        topo = TopographicSimilarity(
            input_metric=hamming,
            messages_metric=editdistance.eval
        )
        topo_value = topo.measure(protocol)
        # print(f'topo = {topo_value:.4f}')
        df.loc[len(df)] = [name, 'topographical similarity', abs(topo_value), seed]


df

sns.set_style("white")
sns.catplot(x='value', y='protocol', col='metric', data=df, kind='box', sharex=False)


